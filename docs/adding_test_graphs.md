# Adding Test Graphs

This document explains how the test graph library is organised, how graphs are stored and loaded, and how to add new graphs — either by modifying the generator or by adding individual custom graphs.

---

## Directory Structure

```
test_graphs/
├── complete/          # K4, K5, K6, K8, K10, K12, K15
├── bipartite/         # K_{2,3} through K_{5,5}
├── grid/              # 2×2 through 5×5
├── cycle/             # C5, C8, ..., C30
├── tree/              # balanced trees (r=2/3, depth=3–5)
├── special/           # Petersen, Dodecahedral, Icosahedral
├── random/            # Erdős–Rényi: n∈{6,8,10,15,20} × d∈{0.2,0.3,0.5,0.7} × 3 instances
├── presets.csv        # named selections (quick, diverse, random, ...)
└── REGISTRY.md        # auto-generated catalog of all graphs with IDs
```

Each graph is stored as a single JSON file at `test_graphs/<category>/<name>.json`. The category subdirectory is just for human organisation — the benchmark system finds graphs by recursively scanning all JSON files.

---

## ID Scheme

Every graph has a numeric ID used for selection. IDs are assigned in clean category blocks:

| ID Range | Category | Examples |
|----------|----------|---------|
| 1–10 | Complete | K4 (1), K5 (2), K6 (3), K8 (4), K10 (5), K12 (6), K15 (7) |
| 11–20 | Bipartite | K_{2,3} (11), K_{3,3} (12), K_{4,4} (14), K_{5,5} (16) |
| 21–30 | Grid | 2×2 (21), 3×3 (22), 4×4 (24), 5×5 (26) |
| 31–40 | Cycle | C5 (31), C8 (32), C10 (33), C15 (34), C20 (35), C30 (36) |
| 41–50 | Tree | balanced trees by branching factor and depth |
| 51–60 | Special | Petersen (51), Dodecahedral (52), Icosahedral (53) |
| 61–99 | Reserved | for future structured categories |
| 100–199 | Random | Erdős–Rényi, grouped by size and density |

IDs are stable — once a graph is assigned an ID, it should not change, because result files store the graph name and comparison across batches depends on consistent naming.

---

## JSON Format

Each graph is stored as:

```json
{
  "id": 1,
  "name": "K4",
  "category": "complete",
  "num_nodes": 4,
  "num_edges": 6,
  "density": 1.0,
  "metadata": {
    "type": "complete",
    "n": 4
  },
  "graph": {
    "directed": false,
    "multigraph": false,
    "graph": {},
    "nodes": [{"id": 0}, {"id": 1}, {"id": 2}, {"id": 3}],
    "edges": [
      {"source": 0, "target": 1},
      {"source": 0, "target": 2},
      ...
    ]
  }
}
```

**Key fields:**

| Field | Description |
|-------|-------------|
| `id` | Unique integer ID used for selection |
| `name` | String name (e.g. `"K4"`, `"random_n10_d0.5_i2"`) — used as `problem_name` in results |
| `category` | Category string — used by qeanalysis to group plots and tables |
| `num_nodes`, `num_edges`, `density` | Scalar properties stored redundantly for quick catalogue access without loading the graph |
| `metadata` | Free-form dict for any additional properties (type, seed, branching factor, etc.) |
| `graph` | NetworkX `node_link_data` format. The edge key is always `"edges"` (not `"links"`) regardless of NetworkX version |

The `graph` field is what `nx.node_link_graph()` reads. The edge key normalization (`"links"` → `"edges"`) is handled automatically by `save_graph()` and `load_graph()` for cross-version compatibility.

---

## Generating the Standard Library

All structured graphs (IDs 1–60) and random graphs (IDs 100–199) are generated by `qebench/graphs.py` via the CLI entry point:

```bash
python -m qebench.graphs          # generate all graphs
python -m qebench.graphs --list   # list existing graphs without regenerating
```

Or from Python:

```python
from qebench.graphs import generate_all
generate_all()
```

This calls each category generator in order and writes the JSON files. If a file already exists it is overwritten with fresh data. At the end it regenerates `REGISTRY.md`.

**Random graph parameters** can be customised:

```bash
python -m qebench.graphs --sizes 10 20 30 --densities 0.2 0.5 0.8 --instances 5
```

This generates random graphs for `n ∈ {10, 20, 30}`, `p ∈ {0.2, 0.5, 0.8}`, with 5 instances each (IDs 100 onward, in order of n → p → instance). The seed for each instance is deterministic: `seed = n * 1000 + int(d * 100) + i`, so regenerating with the same parameters always produces the same graphs.

---

## Adding a New Graph Category

If you want to add a new structured category (e.g. power graphs, Cayley graphs, circulant graphs), follow this pattern:

### 1. Write a generator function in `qebench/graphs.py`

```python
def generate_power_graphs():
    """IDs 061-070: Power graphs G^k."""
    configs = [
        (61, nx.cycle_graph(10), 2, "power_C10_k2"),
        (62, nx.path_graph(8),   2, "power_P8_k2"),
        (63, nx.cycle_graph(15), 3, "power_C15_k3"),
    ]
    graphs = []
    for gid, base, k, name in configs:
        G = nx.power(base, k)
        G = nx.convert_node_labels_to_integers(G)
        save_graph(G, gid, name, "power", {'type': 'power', 'k': k})
        graphs.append((name, G))
    return graphs
```

**Rules:**
- Use IDs from the reserved range 61–99 for new structured categories.
- Always call `nx.convert_node_labels_to_integers()` if your NetworkX generator produces non-integer nodes (grid graphs do this, for example).
- The `category` string you pass to `save_graph()` becomes the subdirectory name and the `category` column in analysis output. Keep it lowercase with no spaces.
- Add meaningful metadata so the `metadata` field is useful for future filtering.

### 2. Add it to `generate_all()`

In the `generate_all()` function, add your generator to the `generators` list:

```python
generators = [
    ("Complete graphs (001-010)", generate_complete_graphs),
    ("Bipartite graphs (011-020)", generate_bipartite_graphs),
    ...
    ("Power graphs (061-070)", generate_power_graphs),   # add here
]
```

### 3. Update `infer_category()` in `qeanalysis/loader.py`

The `infer_category()` function in `qeanalysis/loader.py` infers a graph's category from its `problem_name` string (used for the analysis `category` column at load time). Add a prefix rule for your new category:

```python
for prefix, category in [
    ('bipartite_', 'bipartite'),
    ('grid_',      'grid'),
    ('cycle_',     'cycle'),
    ('tree_',      'tree'),
    ('random_',    'random'),
    ('power_',     'power'),    # add this
]:
    if name.startswith(prefix):
        return category
```

Make sure the prefix matches your naming convention in `generate_power_graphs()`.

### 4. Regenerate

```bash
python -m qebench.graphs
```

This writes the new JSON files and regenerates `REGISTRY.md`.

---

## Adding a Single Custom Graph

To add one specific graph without modifying the generator:

```python
import networkx as nx
from qebench.graphs import save_graph

# Build the graph
G = nx.karate_club_graph()
G = nx.convert_node_labels_to_integers(G)

# Save it — choose an ID in your target range
save_graph(
    G,
    graph_id=61,
    name="karate_club",
    category="social",
    metadata={'type': 'karate_club', 'source': 'Zachary 1977', 'nodes': 34}
)
```

After saving, regenerate the registry:

```python
from qebench.graphs import list_test_graphs, _generate_registry_doc
_generate_registry_doc(list_test_graphs())
```

The graph is now immediately loadable:

```python
from qebench.graphs import load_test_graphs
problems = load_test_graphs("61")    # just this graph
problems = load_test_graphs("61, 1-10")  # this graph + complete graphs
```

---

## Adding a Custom Preset

Presets are named selections defined in `test_graphs/presets.csv`. Each line is:

```
name,selection_string
```

The selection string uses the same syntax as `load_test_graphs()`. Open the file and add your preset:

```csv
name,selection
...
my_preset,61-70, 100-110
stress_test,5-7, 15-16, 130-159
```

Then use it:

```python
from qebench.graphs import load_test_graphs
problems = load_test_graphs("my_preset")
```

**Notes:**
- Preset names are case-sensitive.
- The selection string can contain commas — the parser splits only on the **first** comma in each line to separate the name from the selection.
- `"*"` and `"all"` both load everything.

---

## Loading Graphs

### By ID range

```python
from qebench.graphs import load_test_graphs

problems = load_test_graphs("1-10")         # complete graphs
problems = load_test_graphs("100-199")      # all random graphs
problems = load_test_graphs("1-60, !41-50") # structured, no trees
problems = load_test_graphs("*")            # everything
```

### By preset name

```python
problems = load_test_graphs("quick")     # K4, K5, K6, C5, C8, Petersen
problems = load_test_graphs("diverse")   # one from each category
problems = load_test_graphs("large")     # larger graphs only
problems = load_test_graphs("random")    # IDs 100-199
```

### With size filters

```python
# Only graphs with 10-20 nodes, from the random range
problems = load_test_graphs("100-199", min_nodes=10, max_nodes=20)
```

`load_test_graphs()` returns a list of `(name, nx.Graph)` tuples sorted by ID, ready to pass directly to `run_full_benchmark()`.

---

## Examining the Registry

After generation, `REGISTRY.md` contains a full table of all graphs. You can also query it programmatically:

```python
from qebench.graphs import list_test_graphs

catalog = list_test_graphs()
for entry in catalog:
    print(f"[{entry['id']:3d}] {entry['name']:30s} n={entry['nodes']} d={entry['density']:.3f}")
```

Or from the command line:

```bash
python -m qebench.graphs --list
```

---

## Conventions and Constraints

- **Node labels must be integers starting from 0.** Always call `nx.convert_node_labels_to_integers()` if your source graph uses other labels (tuples for grids, strings for named graphs, etc.). All algorithms assume integer node labels.
- **Graph IDs must be unique.** Check `REGISTRY.md` before assigning a new ID to avoid collisions.
- **Names must be unique.** The name becomes the `problem_name` column in results. Duplicate names across categories would cause ambiguity in analysis.
- **Graphs must be simple and undirected.** The benchmark runner and all algorithms assume undirected, unweighted, no-self-loops graphs. Use `nx.Graph` (not `DiGraph` or `MultiGraph`).
- **Disconnected graphs are allowed** but may cause some algorithms to produce partial or invalid embeddings. The benchmark records whatever is returned and validates it; it does not pre-filter by connectivity.
